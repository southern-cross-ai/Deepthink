# from langchain_community.chat_models import ChatOllama
from langchain_ollama import ChatOllama
'''
llm = ChatOllama(model="llama2", base_url="http://localhost:11434")
response = llm.invoke("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ æ˜¯è°")
print(response.content)
'''

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
# no reply, maybe the format imcompatibility
# 1.å…ˆå°è¯•ä¿®æ”¹prompttemplate
#prompt = ChatPromptTemplate.from_template("è¯·æ ¹æ®ä¸‹é¢çš„ä¸»é¢˜å†™ä¸€ç¯‡å°çº¢ä¹¦è¥é”€çš„çŸ­æ–‡ï¼š {topic}")
'''
from langchain_core.prompts import PromptTemplate
prompt = PromptTemplate.from_template("å¤è¿°ä¸€éä¸»é¢˜ï¼š {topic}")
model = ChatOllama(model="llama2", base_url="http://localhost:11434")
output_parser = StrOutputParser()

chain = prompt | model | output_parser

chain.invoke({"topic": "åº·å¸ˆå‚…ç»¿èŒ¶"})
'''
# 2. åŠ å…¥debug, å…³é—­æµå¼è¾“å‡ºï¼Œæ‰“å°log----å¤±è´¥
from langchain_core.prompts import PromptTemplate
import logging
logging.basicConfig(level=logging.DEBUG)

# prompt = PromptTemplate.from_template("{topic}")
# model = ChatOllama(model="llama2", base_url="http://localhost:11434", streaming=False)
# output_parser = StrOutputParser()

# chain = prompt | model | output_parser

# chain.invoke({"topic": "hi"})
# 3. åˆ†é˜¶æ®µæµ‹è¯•
prompt = PromptTemplate.from_template("{topic}")
model = ChatOllama(model="llama2", base_url="http://localhost:11434", streaming=False)
output_parser = StrOutputParser()

formatted_prompt = prompt.format(topic="hi")
print("ğŸ“¨ Prompt è¾“å‡ºï¼š", formatted_prompt)
#ğŸ“¨ Prompt è¾“å‡ºï¼š hi

llm_response = model.invoke(formatted_prompt)
print("ğŸ¤– LLM åŸå§‹å“åº”ï¼š", llm_response)
#ğŸ¤– LLM åŸå§‹å“åº”ï¼š content="Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?" 
# additional_kwargs={} 
# response_metadata={'model': 'llama2', 'created_at': '2025-04-11T06:17:38.567609324Z', 'done': True, 'done_reason': 'stop', 'total_duration': 209875198048, 'load_duration': 208896110967, 'prompt_eval_count': 21, 'prompt_eval_duration': 376542896, 'eval_count': 26, 'eval_duration': 600786082, 'message': Message(role='assistant', content='', images=None, tool_calls=None), 'model_name': 'llama2'} 
# id='run-75a5f4a2-f279-4e82-8f6f-0ba528083847-0' 
# usage_metadata={'input_tokens': 21, 'output_tokens': 26, 'total_tokens': 47}

parsed = output_parser.invoke(llm_response)
print("ğŸ“¦ æœ€ç»ˆè§£æç»“æœï¼š", parsed)
# ğŸ“¦ æœ€ç»ˆè§£æç»“æœï¼š Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?
