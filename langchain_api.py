from fastapi import FastAPI, Request
from pydantic import BaseModel
from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
import uvicorn
import logging

# âœ… å¯ç”¨æ—¥å¿—æ‰“å°
logging.basicConfig(level=logging.INFO)

# âœ… åˆå§‹åŒ– FastAPI å®ä¾‹
app = FastAPI()

# âœ… å®šä¹‰è¯·æ±‚æ ¼å¼
class ChatRequest(BaseModel):
    topic: str

# âœ… åˆå§‹åŒ– LangChain å…ƒä»¶
prompt = PromptTemplate.from_template("{topic}")
model = ChatOllama(model="llama2", base_url="http://localhost:11434", streaming=False)
parser = StrOutputParser()

@app.post("/chat")
async def chat(req: ChatRequest):
    logging.info(f"ğŸ’¬ æ¥æ”¶åˆ°ç”¨æˆ·è¾“å…¥: {req.topic}")
    formatted = prompt.format(topic=req.topic)
    logging.info(f"ğŸ“¨ Prompt æ¨¡æ¿å: {formatted}")
    llm_result = model.invoke(formatted)
    logging.info(f"ğŸ¤– LLM è¿”å›ï¼š{llm_result}")
    parsed = parser.invoke(llm_result)
    return {"response": parsed}

# âœ… å…è®¸ç›´æ¥è¿è¡Œï¼šuvicorn langchain_api:app --host 0.0.0.0 --port 8000
if __name__ == "__main__":
    uvicorn.run("langchain_api:app", host="0.0.0.0", port=8000, reload=False)